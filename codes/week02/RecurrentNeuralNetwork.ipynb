{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e7e03b",
   "metadata": {},
   "source": [
    "This code serves as an illustrative example of how to implement a recurrent neural network, without regard for efficiency considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0409d0f",
   "metadata": {},
   "source": [
    "## Settings and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a56e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example corpus\n",
    "corpus = \"hello world\"\n",
    "chars = list(set(corpus))\n",
    "data_size, vocab_size = len(corpus), len(chars)\n",
    "\n",
    "# Character to index and index to character mappings\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 100  # Size of the hidden layer of neurons\n",
    "seq_length = 10  # Number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef208daf",
   "metadata": {},
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4270d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01  # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01  # hidden to output\n",
    "bh = np.zeros((hidden_size, 1))  # hidden bias\n",
    "by = np.zeros((vocab_size, 1))  # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # Forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1))  # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)  # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by  # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t], 0])  # softmax (cross-entropy loss)\n",
    "    \n",
    "    # Backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1  # backprop into y\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext  # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh  # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)  # clip to mitigate exploding gradients\n",
    "    \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2101dc70",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b41317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " ello worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldrodworldro worldrodworldro worldro worldrodworldro worldro worldro worldrodworldrodworldrodworldro worldrodwor \n",
      "----\n",
      "iter 0, loss: 0.000014\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldrodworldro worldro worldro worldro worldrodworldro worldro worldro worldro worldrodworldro wor \n",
      "----\n",
      "iter 1000, loss: 0.000014\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldrodworldro worldrodworldro worldrodworldro worldrodworldro worldro worldrodworldrodworldrodworldrodworldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 2000, loss: 0.000014\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldrodworldrodworldro worldro worldrodworldrodworldro worldrodworldro worldro worldrodworldro worldro worldro worldrodworldrodworldro worldro wor \n",
      "----\n",
      "iter 3000, loss: 0.000013\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldrodworldro worldrodworldro worldro worldrodworldro worldro worldro worldrodworldro worldrodworldro worldrodworldrodwor \n",
      "----\n",
      "iter 4000, loss: 0.000013\n",
      "----\n",
      " ello worldrodworldro worldro worldro worldro worldro worldro worldro worldrodworldro worldrodworldro worldrodworldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 5000, loss: 0.000013\n",
      "----\n",
      " ello worldro worldro worldrodworldro worldro worldrodworldro worldrodworldrodworldro worldrodworldro worldro worldro worldro worldrodworldrodworldro worldrodworldro worldro worldro worldrodworldro wor \n",
      "----\n",
      "iter 6000, loss: 0.000013\n",
      "----\n",
      " ello worldro worldro worldro worldrodworldrodworldrodworldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 7000, loss: 0.000013\n",
      "----\n",
      " ello worldrodworldro worldro worldro worldro worldro worldrodworldrodworldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 8000, loss: 0.000013\n",
      "----\n",
      " ello worldro worldrodworldrodworldro worldrodworldrodworldrodworldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldrodworldro worldro worldro worldrodworldro wor \n",
      "----\n",
      "iter 9000, loss: 0.000013\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldro worldro worldrodworldro worldrodworldro worldro worldro wor \n",
      "----\n",
      "iter 10000, loss: 0.000013\n",
      "----\n",
      " ello worldro worldrodworldro worldro worldro worldrodworldro worldro worldro worldrodworldrodworldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldrodwor \n",
      "----\n",
      "iter 11000, loss: 0.000013\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldrodworldrodworldrodworldro worldro worldrodworldrodworldro worldrodworldro worldrodworldro worldro worldro worldrodworldro worldro worldlodworldro wor \n",
      "----\n",
      "iter 12000, loss: 0.000012\n",
      "----\n",
      " ello worldrodworldrodworldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldrodworldro worldro worldrodworldro worldrodworldro worldro worldro wor \n",
      "----\n",
      "iter 13000, loss: 0.000012\n",
      "----\n",
      " ello worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldrodworldro worldrodworldro worldro worldro worldrodworldro worldro worldrodwor \n",
      "----\n",
      "iter 14000, loss: 0.000012\n",
      "----\n",
      " ello worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldrodworldrodworldrodworldro worldrodworldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 15000, loss: 0.000012\n",
      "----\n",
      " ello worldro worldro worldrodworldro worldrodworldrodworldrodworldro worldrodworldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldrodwor \n",
      "----\n",
      "iter 16000, loss: 0.000012\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldrodworldro worldro worldro worldro worldrodworldrodworldro worldro worldro worldrodworldro wor \n",
      "----\n",
      "iter 17000, loss: 0.000012\n",
      "----\n",
      " ello worldro worldro worldro worldrodworldro worldro worldrodworldro worldrodworldrodworldro worldro worldro worldrodworldrodworldro worldrodworldro worldro worldro worldro worldro worldro worldrodwor \n",
      "----\n",
      "iter 18000, loss: 0.000012\n",
      "----\n",
      " ello worldro worldrodworldrodworldro worldro worldrodworldro worldro worldrodworldro worldrodworldro worldrodworldrodworldrodworldro worldro worldro worldrodworldro worldro worldro worldrodworldro wor \n",
      "----\n",
      "iter 19000, loss: 0.000012\n",
      "----\n",
      " ello worldrodworldro worldro worldro worldro worldro worldrodworldrodworldro worldro worldrodworldro worldro worldro worldro worldro worldro worldrodworldrodworldro worldro worldro worldrodworldro wor \n",
      "----\n",
      "iter 20000, loss: 0.000012\n",
      "----\n",
      " ello worldro worldro worldro worldro worldrodworldro worldrodworldrodworldrodworldro worldro worldrodworldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldrodworldrodwor \n",
      "----\n",
      "iter 21000, loss: 0.000012\n",
      "----\n",
      " ello worldrodworldro worldro worldro worldrodworldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldrodworldrodworldrodwor \n",
      "----\n",
      "iter 22000, loss: 0.000011\n",
      "----\n",
      " ello worldro worldrodworldro worldrodworldro worldrodworldro worldro worldro worldro worldro worldrodworldrodworldro worldro worldro worldro worldro worldrodworldro worldrodworldro worldro worldro wor \n",
      "----\n",
      "iter 23000, loss: 0.000011\n",
      "----\n",
      " ello worldro worldrodworldrodworldro worldro worldro worldro worldrodworldrodworldro worldro worldrodworldrodworldro worldro worldro worldrodworldro worldro worldro worldro worldrodworldro worldro wor \n",
      "----\n",
      "iter 24000, loss: 0.000011\n",
      "----\n",
      " ello worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldrodworldrodworldro worldro worldro worldro worldro worldro worldrodworldro worldro worldrodworldrodworldro worldro wor \n",
      "----\n",
      "iter 25000, loss: 0.000011\n",
      "----\n",
      " ello worldro worldrodworldro worldro worldrodworldro worldro worldrodworldro worldro worldro worldro worldrodworldro worldro worldro worldrodworldro worldro worldrodworldro worldro worldrodworldrodwor \n",
      "----\n",
      "iter 26000, loss: 0.000011\n",
      "----\n",
      " ello worldro worldro worldrodworldro worldro worldrodworldro worldrodworldrodworldro worldro worldro worldro worldro worldro worldrodworldro worldrodworldro worldro worldro worldro worldrodworldro wor \n",
      "----\n",
      "iter 27000, loss: 0.000011\n",
      "----\n",
      " ello worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodwor \n",
      "----\n",
      "iter 28000, loss: 0.000011\n",
      "----\n",
      " ello worldrodworldrodworldro worldrodworldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldro worldrodworldro worldro worldro wor \n",
      "----\n",
      "iter 29000, loss: 0.000011\n",
      "----\n",
      " ello worldro worldro worldrodworldrodworldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldrodworldro worldrodworldro worldro wor \n",
      "----\n",
      "iter 30000, loss: 0.000011\n",
      "----\n",
      " ello worldro worldrodworldrodworldro worldro worldrodworldro worldrodworldro worldro worldro worldro worldrodworldro worldro worldro worldro worldrodworldro worldrodworldrodworldro worldro worldro wor \n",
      "----\n",
      "iter 31000, loss: 0.000011\n",
      "----\n",
      " ello worldro worldro worldrodworldro worldro worldro worldrodworldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 32000, loss: 0.000011\n",
      "----\n",
      " ello worldro worldro worldro worldro worldrodworldrodworldrodworldro worldro worldro worldro worldrodworldro worldrodworldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldro wor \n",
      "----\n",
      "iter 33000, loss: 0.000011\n",
      "----\n",
      " ello worldrodworldro worldro worldro worldro worldrodworldro worldro worldrodworldrodworldro worldro worldrodworldrodworldrodworldro worldro worldro worldro worldro worldro worldro worldrodworldrodwor \n",
      "----\n",
      "iter 34000, loss: 0.000011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " ello worldro worldro worldro worldrodworldrodworldro worldrodworldro worldrodworldro worldrodworldro worldro worldrodworldrodworldro worldrodworldrodworldro worldrodworldrodworldro worldro worldrodwor \n",
      "----\n",
      "iter 35000, loss: 0.000010\n",
      "----\n",
      " ello worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldrodworldro worldro worldro worldro worldro worldro worldro worldrodworldrodwor \n",
      "----\n",
      "iter 36000, loss: 0.000010\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldrodworldro worldro worldrodworldro worldro worldrodwor \n",
      "----\n",
      "iter 37000, loss: 0.000010\n",
      "----\n",
      " ello worldro worldrodworldro worldro worldro worldrodworldro worldrodworldrodworldro worldro worldro worldro worldro worldro worldro worldrodworldrodworldro worldrodworldrodworldro worldrodworldrodwor \n",
      "----\n",
      "iter 38000, loss: 0.000010\n",
      "----\n",
      " ello worldrodworldro worldrodworldro worldro worldro worldro worldrodworldrodworldrodworldro worldro worldro worldro worldro worldro worldro worldrodworldrodworldrodworldro worldrodworldrodworldrodwor \n",
      "----\n",
      "iter 39000, loss: 0.000010\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldrodworldrodworldro wor \n",
      "----\n",
      "iter 40000, loss: 0.000010\n",
      "----\n",
      " ello worldrodworldro worldro worldrodworldro worldro worldrodworldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldrodworldro worldro worldro wor \n",
      "----\n",
      "iter 41000, loss: 0.000010\n",
      "----\n",
      " ello worldro worldro worldro worldro worldrodworldro worldro worldrodworldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldrodworldro worldro worldro worldro wor \n",
      "----\n",
      "iter 42000, loss: 0.000010\n",
      "----\n",
      " ello worldrodworldrodworldro worldro worldrodworldro worldrodworldrodworldro worldro worldro worldro worldrodworldro worldro worldro worldro worldrodworldrodworldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 43000, loss: 0.000010\n",
      "----\n",
      " ello worldrodworldrodworldro worldro worldro worldro worldrodworldrodworldro worldro worldro worldro worldrodworldrodworldrodworldro worldro worldro worldro worldro worldro worldrodworldro worldrodwor \n",
      "----\n",
      "iter 44000, loss: 0.000010\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldrodworldro worldro worldro worldro worldrodworldrodworldro worldrodworldro worldrodworldro worldro worldro worldro wor \n",
      "----\n",
      "iter 45000, loss: 0.000010\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldro worldro worldrodworldrodworldro worldro wor \n",
      "----\n",
      "iter 46000, loss: 0.000010\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldro worldrodworldro worldro worldro worldro wor \n",
      "----\n",
      "iter 47000, loss: 0.000010\n",
      "----\n",
      " ello worldrodworldro worldro worldro worldrodworldro worldrodworldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldro worldro wor \n",
      "----\n",
      "iter 48000, loss: 0.000010\n",
      "----\n",
      " ello worldrodworldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro wor \n",
      "----\n",
      "iter 49000, loss: 0.000010\n",
      "----\n",
      " ello worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldrodworldrodworldro worldro worldro worldrodworldro worldro worldro worldrodworldro worldro worldro worldro wor \n",
      "----\n",
      "iter 50000, loss: 0.000009\n",
      "----\n",
      " ello worldro worldro worldro worldrodworldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 51000, loss: 0.000009\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldrodworldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldrodworldro worldrodworldro worldrodworldrodwor \n",
      "----\n",
      "iter 52000, loss: 0.000009\n",
      "----\n",
      " ello worldro worldro worldrodworldrodworldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldrodworldro worldro worldro worldro worldro worldrodwor \n",
      "----\n",
      "iter 53000, loss: 0.000009\n",
      "----\n",
      " ello worldrodworldro worldrodworldro worldro worldro worldrodworldro worldrodworldro worldro worldrodworldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 54000, loss: 0.000009\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldrodworldro wor \n",
      "----\n",
      "iter 55000, loss: 0.000009\n",
      "----\n",
      " ello worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldrodworldrodworldrodworldro wor \n",
      "----\n",
      "iter 56000, loss: 0.000009\n",
      "----\n",
      " ello worldro worldrodworldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldrodworldro worldrodwor \n",
      "----\n",
      "iter 57000, loss: 0.000009\n",
      "----\n",
      " ello worldro worldro worldro worldrodworldro worldrodworldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldrodworldrodworldro worldrodwor \n",
      "----\n",
      "iter 58000, loss: 0.000009\n",
      "----\n",
      " ello worldro worldrodworldro worldro worldro worldro worldrodworldro worldro worldro worldro worldrodworldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldrodworldro wor \n",
      "----\n",
      "iter 59000, loss: 0.000009\n",
      "----\n",
      " ello worldro worldrodworldro worldro worldro worldro worldrodworldrodworldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro wor \n",
      "----\n",
      "iter 60000, loss: 0.000009\n",
      "----\n",
      " ello worldro worldrodworldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldro worldrodworldro worldro worldro wor \n",
      "----\n",
      "iter 61000, loss: 0.000009\n",
      "----\n",
      " ello worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldrodworldro worldro worldrodworldro worldro wor \n",
      "----\n",
      "iter 62000, loss: 0.000009\n",
      "----\n",
      " ello worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 63000, loss: 0.000009\n",
      "----\n",
      " ello worldrodworldro worldro worldro worldro worldrodworldro worldrodworldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 64000, loss: 0.000009\n",
      "----\n",
      " ello worldrodworldro worldro worldro worldro worldrodworldrodworldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldrodworldro wor \n",
      "----\n",
      "iter 65000, loss: 0.000009\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldrodworldrodworldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldrodworldrodworldro worldrodworldrodwor \n",
      "----\n",
      "iter 66000, loss: 0.000009\n",
      "----\n",
      " ello worldrodworldro worldro worldrodworldro worldrodworldro worldrodworldrodworldrodworldro worldrodworldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldrodworldrodwor \n",
      "----\n",
      "iter 67000, loss: 0.000009\n",
      "----\n",
      " ello worldro worldrodworldrodworldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodwor \n",
      "----\n",
      "iter 68000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 69000, loss: 0.000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " ello worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldrodwor \n",
      "----\n",
      "iter 70000, loss: 0.000008\n",
      "----\n",
      " ello worldrodworldro worldrodworldrodworldro worldro worldro worldrodworldro worldrodworldrodworldro worldro worldrodworldrodworldro worldro worldrodworldrodworldro worldrodworldrodworldro worldro wor \n",
      "----\n",
      "iter 71000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldro worldrodworldro worldro worldrodworldrodworldro worldro worldrodworldrodworldro worldro worldro worldro worldro worldro worldro worldro worldrodworldrodworldro worldrodwor \n",
      "----\n",
      "iter 72000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldrodworldrodworldro worldro worldrodworldro worldrodworldro worldrodwor \n",
      "----\n",
      "iter 73000, loss: 0.000008\n",
      "----\n",
      " ello worldrodworldrodworldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldrodworldrodworldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro wor \n",
      "----\n",
      "iter 74000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldrodworldro worldrodworldro worldrodworldro worldro worldrodworldro worldro worldrodworldrodworldro worldrodworldro worldrodworldro worldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 75000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldro worldrodworldrodworldro worldro worldro worldro worldrodworldro worldro worldrodworldrodworldrodworldro worldro worldro worldrodworldro worldro worldro worldrodworldro wor \n",
      "----\n",
      "iter 76000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldrodwor \n",
      "----\n",
      "iter 77000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldrodworldro worldrodworldrl worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodwor \n",
      "----\n",
      "iter 78000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 79000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldrodworldro worldro worldro worldro worldrodworldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldrodwor \n",
      "----\n",
      "iter 80000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldlodworldro worldrodworldro worldro worldrodworldro worldro worldrodwor \n",
      "----\n",
      "iter 81000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldro worldro worldrodworldro worldro worldro worldrodworldro worldro worldrodworldro worldro worldro worldrodworldro worldro worldrodworldro worldrodworldrodworldro worldro wor \n",
      "----\n",
      "iter 82000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 83000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldro worldro worldrodworldrodworldro worldro worldro worldrodworldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldrodwor \n",
      "----\n",
      "iter 84000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldro worldrodworldro worldro worldrodworldro worldrodworldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro wor \n",
      "----\n",
      "iter 85000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldro worldro worldrodworldro worldrodworldro worldrodworldrodworldrodworldro worldrodworldrodworldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 86000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldrodworldro worldrodworldro worldlo worldro worldro worldrodworldrodworldro worldro wor \n",
      "----\n",
      "iter 87000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 88000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldrodworldro worldro worldro worldro worldro worldro worldrodworldro worldro wor \n",
      "----\n",
      "iter 89000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldrodworldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldrodworldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro wor \n",
      "----\n",
      "iter 90000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 91000, loss: 0.000008\n",
      "----\n",
      " ello worldro worldro worldro worldro worldrodworldro worldrodworldro worldrodworldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 92000, loss: 0.000007\n",
      "----\n",
      " ello worldro worldro worldro worldrodworldro worldro worldro worldrodworldrodworldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldro worldrodworldro worldro worldro worldro wor \n",
      "----\n",
      "iter 93000, loss: 0.000007\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldrodworldro worldro wor \n",
      "----\n",
      "iter 94000, loss: 0.000007\n",
      "----\n",
      " ello worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro wor \n",
      "----\n",
      "iter 95000, loss: 0.000007\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro wor \n",
      "----\n",
      "iter 96000, loss: 0.000007\n",
      "----\n",
      " ello worldrodworldrodworldro worldro worldro worldro worldrodworldro worldrodworldro worldrodworldrodworldro worldrodworldro worldro worldrodworldro worldro worldrodworldro worldro worldro worldro wor \n",
      "----\n",
      "iter 97000, loss: 0.000007\n",
      "----\n",
      " ello worldro worldrodworldro worldro worldro worldro worldrodworldro worldro worldro worldro worldrodworldrodworldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldrodwor \n",
      "----\n",
      "iter 98000, loss: 0.000007\n",
      "----\n",
      " ello worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro worldro worldro worldro worldro worldro worldro worldro worldro worldro worldrodworldro wor \n",
      "----\n",
      "iter 99000, loss: 0.000007\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "n, p = 0, 0\n",
    "hprev = np.zeros((hidden_size, 1))  # reset RNN memory\n",
    "while n < 100000:\n",
    "    # Prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(corpus) or n == 0: \n",
    "        hprev = np.zeros((hidden_size, 1))  # reset RNN memory\n",
    "        p = 0  # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in corpus[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in corpus[p+1:p+seq_length+1]]\n",
    "\n",
    "    # Sample from the model now and then\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # Forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    if n % 1000 == 0: print('iter %d, loss: %f' % (n, loss))  # print progress\n",
    "    \n",
    "    # Perform parameter update with gradient descent\n",
    "    for param, dparam in zip([Wxh, Whh, Why, bh, by], \n",
    "                             [dWxh, dWhh, dWhy, dbh, dby]):\n",
    "        param -= learning_rate * dparam\n",
    "\n",
    "    p += seq_length  # move data pointer\n",
    "    n += 1  # iteration counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a36203",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Once the model is trained, you can use it to generate new text sequences based on a given seed input. The model predicts the next character at each step and uses this prediction as the input for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cb402d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ello world\n"
     ]
    }
   ],
   "source": [
    "h_prev = np.zeros((hidden_size, 1)) # Reset the hidden state\n",
    "\n",
    "seed_ix = char_to_ix['h']\n",
    "n = 10  # Number of characters to generate\n",
    "\n",
    "# Correct function call\n",
    "generated_sequence = sample(h_prev, seed_ix, n)\n",
    "\n",
    "# Convert the generated sequence of indices back to characters\n",
    "generated_text = ''.join(ix_to_char[ix] for ix in generated_sequence)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e9c2ea",
   "metadata": {},
   "source": [
    "This function generates text by predicting one character at a time and using the predicted character as the input for the next prediction. The seed_ix parameter is the starting character index, and n is the number of characters to generate. The output is a string of generated characters based on learned patterns in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e36293b",
   "metadata": {},
   "source": [
    "## Using language models to generate probability and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314793b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from collections import defaultdict\n",
    "\n",
    "#Roberta and Gpt2 use bype-level BPE\n",
    "def init_model(model_name):\n",
    "    if model_name == \"xlnet\":\n",
    "        pretrained_name = 'xlnet-base-cased'\n",
    "        tokenizer = XLNetTokenizer.from_pretrained(pretrained_name)\n",
    "        model = XLNetModel.from_pretrained(pretrained_name, output_hidden_states=True, output_attentions=True)\n",
    "        model_lm = XLNetLMHeadModel.from_pretrained(pretrained_name).eval()\n",
    "    elif model_name == \"distillbert\":\n",
    "        pretrained_name = 'distilbert-base-cased'\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(pretrained_name)\n",
    "        model = DistilBertModel.from_pretrained(pretrained_name, output_hidden_states=True, output_attentions=True)\n",
    "        model_lm = DistilBertForMaskedLM.from_pretrained(pretrained_name).eval()\n",
    "    elif model_name == \"bert\":\n",
    "        pretrained_name = 'bert-base-uncased'\n",
    "        tokenizer = BertTokenizer.from_pretrained(pretrained_name)\n",
    "        model = BertModel.from_pretrained(pretrained_name, output_hidden_states=True, output_attentions=True)\n",
    "        model_lm = BertForMaskedLM.from_pretrained(pretrained_name).eval()\n",
    "    elif model_name == \"bertlarge\":\n",
    "        pretrained_name = 'bert-large-uncased'\n",
    "        tokenizer = BertTokenizer.from_pretrained(pretrained_name)\n",
    "        model = BertModel.from_pretrained(pretrained_name, output_hidden_states=True, output_attentions=True)\n",
    "        model_lm = BertForMaskedLM.from_pretrained(pretrained_name).eval()\n",
    "    elif model_name == \"roberta\":\n",
    "        pretrained_name = 'roberta-base'\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(pretrained_name)\n",
    "        model = BertModel.from_pretrained(pretrained_name, output_hidden_states=True, output_attentions=True)\n",
    "        model_lm = RobertaForMaskedLM.from_pretrained(pretrained_name).eval()\n",
    "    elif model_name == \"gpt\":\n",
    "        pretrained_name = 'openai-gpt'\n",
    "        tokenizer = AutoTokenizer.from_pretrained(pretrained_name)\n",
    "        model = OpenAIGPTModel.from_pretrained(pretrained_name, output_hidden_states=True, output_attentions=True)\n",
    "        model_lm = OpenAIGPTLMHeadModel.from_pretrained(pretrained_name).eval()\n",
    "    elif model_name == \"gpt2\":\n",
    "        pretrained_name = 'gpt2'\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(pretrained_name)\n",
    "        model = GPT2Model.from_pretrained(pretrained_name, output_hidden_states=True, output_attentions=True)\n",
    "        model_lm = GPT2LMHeadModel.from_pretrained(pretrained_name).eval()\n",
    "    elif model_name == \"gpt2large\":\n",
    "        pretrained_name = 'gpt2-large'\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(pretrained_name)\n",
    "        model = GPT2Model.from_pretrained(pretrained_name, output_hidden_states=True, output_attentions=True)\n",
    "        model_lm = GPT2LMHeadModel.from_pretrained(pretrained_name).eval()\n",
    "    else:\n",
    "        logger.error(\"unsupported model: {}\".format(model_name))\n",
    "\n",
    "    return tokenizer, model, model_lm\n",
    "\n",
    "def match_piece_to_word(piece, word):\n",
    "    mapping = defaultdict(list)\n",
    "    word_index = 0\n",
    "    piece_index = 0\n",
    "    while (word_index < len(word.split()) and piece_index < len(piece)):\n",
    "        if piece[piece_index] != '[UNK]':\n",
    "            mid = piece[piece_index].strip('Ġ').strip('▁').strip('##')\n",
    "            mid = mid.replace('</w>', '')\n",
    "            t = len(mid)\n",
    "        else:\n",
    "            t = 1\n",
    "        while (piece_index + 1 < len(piece) and t<len(word.split()[word_index])):\n",
    "            mapping[word_index].append(piece_index)\n",
    "            piece_index += 1\n",
    "            if piece[piece_index] != '[UNK]':\n",
    "                mid = piece[piece_index].strip('Ġ').strip('▁').strip('##')\n",
    "                mid = mid.replace('</w>', '')\n",
    "                t += len(mid)\n",
    "            else:\n",
    "                t += 1\n",
    "        try:\n",
    "            assert(t == len(word.split()[word_index]))\n",
    "        except:\n",
    "            print(word)\n",
    "            print(piece)\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "        mapping[word_index].append(piece_index)\n",
    "        word_index += 1\n",
    "\n",
    "        piece_index += 1\n",
    "    return mapping\n",
    "\n",
    "def convert_logits_to_probs(logits, input_ids):\n",
    "    \"\"\"\"\n",
    "    input:\n",
    "        logits: (1, n_word, n_vocab), GPT2 outputed logits of each word\n",
    "        input_inds: (1, n_word), the word id in vocab\n",
    "    output: probs: (1, n_word), the softmax probability of each word\n",
    "    \"\"\"\n",
    "\n",
    "    probs = F.softmax(logits[0], dim=1)\n",
    "    n_word = input_ids.shape[1]\n",
    "    res = []\n",
    "    for i in range(n_word):\n",
    "        res.append(probs[i, input_ids[0][i]].item())\n",
    "    return np.array(res).reshape(1, n_word)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    parameters\n",
    "    inputfile: sentences with target word\n",
    "    \n",
    "    '''\n",
    "    ind1 = -2  # index for the target word\n",
    "\n",
    "    inputfile = 'abc.txt'\n",
    "\n",
    "    model_names = [\"xlnet\", \"distillbert\", \"bert\", \"bertlarge\", \"roberta\", \"gpt\", \"gpt2\", \"gpt2large\"]\n",
    "    for model_name in model_names:\n",
    "        print(model_name)\n",
    "\n",
    "        out = [] \n",
    "        for input in open(inputfile):\n",
    "            input_sent = input.strip()\n",
    "\n",
    "            tokenizer, model, model_lm = init_model(model_name)\n",
    "            input_ids = tokenizer.encode(input_sent, return_tensors = \"pt\")\n",
    "            tok_input = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "            print(input_ids)\n",
    "            print(tok_input)\n",
    "            if model_name in [\"xlnet\"]:\n",
    "                tok_input = tok_input[0:-2]\n",
    "                input_ids = input_ids[:,0:-2]\n",
    "            elif model_name in [\"distillbert\", \"bert\", \"bertlarge\", \"roberta\"]:\n",
    "                tok_input = tok_input[1:-1]\n",
    "                input_ids = input_ids[:,1:-1]\n",
    "            \n",
    "            tok_sent = ' '.join(tok_input).replace('Ġ', '').replace('▁', '').replace('##', '').replace('</w>', '')\n",
    "            word_piece_mapping = match_piece_to_word(tok_input, input_sent)\n",
    "            # print(word_piece_mapping)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids)\n",
    "                logits = model_lm(input_ids)[0]\n",
    "            hidden_states = outputs['hidden_states']\n",
    "            # print(len(hidden_states))\n",
    "            prob = convert_logits_to_probs(logits, input_ids)[0]\n",
    "            # print(len(prob), prob)\n",
    "            prob1 = 1\n",
    "  \n",
    "            for i in word_piece_mapping[len(input_sent.split())+ind1]:\n",
    "                prob1 *= prob[i]\n",
    "\n",
    "            for i in range(len(outputs['hidden_states'])): #layers\n",
    "                vec1 = outputs['hidden_states'][i][0, word_piece_mapping[len(input_sent.split())+ind1], :].detach().numpy().mean(axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
