{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b886226",
   "metadata": {},
   "source": [
    "Using co-occurrence to calculate word vectors involves capturing the context in which words appear within a corpus of text to represent words as vectors. This approach is based on the distributional hypothesis, which suggests that words that occur in similar contexts tend to have similar meanings. Here's a simplified explanation of how to use co-occurrence to calculate word vectors:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f28b0",
   "metadata": {},
   "source": [
    "Choose Your Corpus: The first step is to select a corpus of text. The choice of corpus is crucial as it determines the context in which words are used. The corpus should be large and diverse enough to capture a wide range of word contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2edd4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Zhangsan likes computer!\",\n",
    "    \"Lisi loves food.\",\n",
    "    \"food is his thing?\",\n",
    "    \"Wangwu loves food;\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b682bf49",
   "metadata": {},
   "source": [
    "Preprocessing the corpus by removing punctuation, tokenizing strings into words and converting all words to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "075bd265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['zhangsan', 'likes', 'computer'],\n",
       " ['lisi', 'loves', 'food'],\n",
       " ['food', 'is', 'his', 'thing'],\n",
       " ['wangwu', 'loves', 'food']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuations = \".,;:!?\"\n",
    "\n",
    "def remove_punctuations(docs):\n",
    "    for i in range(len(docs)):\n",
    "        for punctuation in punctuations:\n",
    "            docs[i] = docs[i].replace(punctuation, \"\")\n",
    "    return docs\n",
    "\n",
    "def tokenize(docs):\n",
    "    docs_tokenized = []\n",
    "    for doc in docs:\n",
    "        docs_tokenized.append(doc.lower().split(\" \"))\n",
    "    return docs_tokenized\n",
    "\n",
    "docs_repunc = remove_punctuations(docs)\n",
    "\n",
    "docs_tokenized = tokenize(docs_repunc)\n",
    "\n",
    "docs_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc7fca",
   "metadata": {},
   "source": [
    "Define the Context of a Word: Decide on the size of the context windowâ€”the number of words before and after a target word that will be considered its context. A smaller window size might capture more syntactic relationships, while a larger window size might capture more semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8475a218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thing': 0,\n",
       " 'loves': 1,\n",
       " 'is': 2,\n",
       " 'wangwu': 3,\n",
       " 'likes': 4,\n",
       " 'computer': 5,\n",
       " 'food': 6,\n",
       " 'his': 7,\n",
       " 'zhangsan': 8,\n",
       " 'lisi': 9}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##window size = len(vocabulary)\n",
    "\n",
    "def build_corpus(docs_tokenized):\n",
    "    words_set = set()\n",
    "    for doc in docs_tokenized:\n",
    "        for word in doc:\n",
    "            words_set.add(word)\n",
    "    res = {}\n",
    "    for i, word in enumerate(list(words_set)):\n",
    "        res[word] = i\n",
    "    return res    \n",
    "\n",
    "corpus = build_corpus(docs_tokenized)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd2442",
   "metadata": {},
   "source": [
    "Create the Co-occurrence Matrix: Construct a matrix where each row represents a target word and each column represents a context word (or vice versa). The value at each position in the matrix (i, j) represents the frequency with which word i occurs in the context of word j. This frequency can be raw count, or it can be weighted in various ways to reduce the impact of very common words or to emphasize closer word pairs within the context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca693606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.33333333 0.33333333\n",
      "  0.         0.         0.33333333 0.        ]\n",
      " [0.         0.33333333 0.         0.         0.         0.\n",
      "  0.33333333 0.         0.         0.33333333]\n",
      " [0.25       0.         0.25       0.         0.         0.\n",
      "  0.25       0.25       0.         0.        ]\n",
      " [0.         0.33333333 0.         0.33333333 0.         0.\n",
      "  0.33333333 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_tf(docs, corpus):\n",
    "    res = []\n",
    "    for doc in docs:\n",
    "        freq = [0] * len(corpus)\n",
    "        for word in doc:\n",
    "            freq[corpus[word]] += 1\n",
    "        total_count = sum(freq)\n",
    "        for i in range(len(freq)):\n",
    "            freq[i] /= total_count\n",
    "        res.append(freq)\n",
    "    return res\n",
    "\n",
    "ans = get_tf(docs_tokenized, corpus)\n",
    "print(np.matrix(ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8b7021",
   "metadata": {},
   "source": [
    "Transform the Co-occurrence Matrix: Sometimes, the raw co-occurrence frequencies are transformed to improve the quality of the resulting vectors. Techniques such as Positive Pointwise Mutual Information (PPMI) or transformations like TF-IDF (Term Frequency-Inverse Document Frequency) can be used to adjust the values in the matrix to better capture meaningful associations between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c5fd233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.5773502691896257, 0.5773502691896257, 0.0, 0.0, 0.5773502691896257, 0.0]\n",
      "[0.0, 0.5534923152870045, 0.0, 0.0, 0.0, 0.0, 0.4480997313625987, 0.0, 0.0, 0.7020348194149619]\n",
      "[0.5417361046803605, 0.0, 0.5417361046803605, 0.0, 0.0, 0.0, 0.3457831381910465, 0.5417361046803605, 0.0, 0.0]\n",
      "[0.0, 0.5534923152870044, 0.0, 0.7020348194149618, 0.0, 0.0, 0.4480997313625986, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def normalize(vector):\n",
    "    norm_constant = np.sqrt(sum(num ** 2 for num in vector))\n",
    "    for i in range(len(vector)):\n",
    "        vector[i] /= norm_constant\n",
    "    return vector\n",
    "\n",
    "def count(docs):\n",
    "    res = []\n",
    "    for doc in docs:\n",
    "        res += doc\n",
    "    return Counter(res)\n",
    "\n",
    "def get_tfidf(docs, corpus):\n",
    "    tf = get_tf(docs, corpus)\n",
    "    idf = get_idf(docs, corpus)\n",
    "    for i, doc in enumerate(tf):\n",
    "        for j in range(len(doc)):\n",
    "            doc[j] *= idf[j]\n",
    "        tf[i] = normalize(doc)\n",
    "    return tf\n",
    "\n",
    "def get_idf(docs, corpus):\n",
    "    n_docs = len(docs)\n",
    "    counter = count(docs)\n",
    "    res = [0] * len(corpus)\n",
    "    for word, i in corpus.items():\n",
    "        res[i] = 1 + np.log((n_docs + 1) / (1 + counter[word]))\n",
    "    return res\n",
    "\n",
    "tf_idf = get_tfidf(docs_tokenized, corpus)\n",
    "for doc in tf_idf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379e028",
   "metadata": {},
   "source": [
    "Reduce Dimensionality: The co-occurrence matrix can be very large and sparse, especially for a large corpus with a vast vocabulary. Dimensionality reduction techniques, such as Singular Value Decomposition (SVD), Principal Component Analysis (PCA), or non-negative matrix factorization (NMF), can be applied to the matrix to produce a more compact, dense representation of word vectors. This step reduces the size of the vector space while preserving as much of the significant structural information as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5ae6e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd(co_occurrence_matrix):\n",
    "    U, sigma, Vt = np.linalg.svd(co_occurrence_matrix, full_matrices=False)\n",
    "    n_components = 2  # Number of components to keep\n",
    "    reduced_matrix_svd = U[:, :n_components] * sigma[:n_components]\n",
    "    return reduced_matrix_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baf88836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ],\n",
       "       [-0.83528054,  0.        ],\n",
       "       [-0.43968363,  0.        ],\n",
       "       [-0.83528054,  0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfae1eb",
   "metadata": {},
   "source": [
    "Simpler implementation from scikit-learn toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18935387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57735027 0.         0.         0.         0.57735027 0.\n",
      " 0.         0.         0.         0.57735027]\n",
      "[0.         0.44809973 0.         0.         0.         0.70203482\n",
      " 0.55349232 0.         0.         0.        ]\n",
      "[0.         0.34578314 0.5417361  0.5417361  0.         0.\n",
      " 0.         0.5417361  0.         0.        ]\n",
      "[0.         0.44809973 0.         0.         0.         0.\n",
      " 0.55349232 0.         0.70203482 0.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "sklearn_tfidf = vectorizer.fit_transform(docs).toarray()\n",
    "\n",
    "for doc in sklearn_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc67ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
