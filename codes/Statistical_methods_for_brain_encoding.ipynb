{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5877b74",
   "metadata": {},
   "source": [
    "## Statistical methods for brain encoding\n",
    "\n",
    "This tutorial is designed for the UCAS-AI2024 course titled \"Language and Brain Data Processing,\" focusing on statistical methods for brain encoding. Throughout this tutorial, we will delve into the processes involved in brain encoding, particularly examining how syntax is encoded within the brain. Key areas covered will include methods to quantify syntactic complexity, aligning stimuli with BOLD signals using the hemodynamic response function (HRF), and correlating syntactic complexity with fMRI data through General Linear Models (GLM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52ed60",
   "metadata": {},
   "source": [
    "### syntactic complexity quantifications using node count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01909633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing node count... Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "\n",
    "def node_count_bu(in_file):\n",
    "    \"\"\"\n",
    "    computing the bottom-up node count by counting \n",
    "    the number of right parenthesis behind each word\n",
    "    \"\"\"\n",
    "    with open(in_file, 'r', encoding='utf-8') as rf:\n",
    "        embed = []\n",
    "        for line in rf:\n",
    "            line = line.strip().split()\n",
    "            i = 0\n",
    "            while i < len(line):\n",
    "                if line[i].encode().isalpha() and line[i+2] == ')':\n",
    "                    count = 0\n",
    "                    i += 2\n",
    "                    while i<len(line) and line[i] == ')':\n",
    "                        count += 1\n",
    "                        i += 1\n",
    "                    embed.append(count)\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "    return np.array(embed).reshape(len(embed), 1)\n",
    "\n",
    "def node_count_td(in_file):\n",
    "    \"\"\"\n",
    "    computing the top-down node count by counting \n",
    "    the number of left parenthesis between each word\n",
    "    and the nearest right parenthesis before it.\n",
    "    \"\"\"\n",
    "    with open(in_file, 'r', encoding='utf-8') as rf:\n",
    "        embed = []\n",
    "        count = 0\n",
    "        for line in rf:\n",
    "            line = line.strip().split()\n",
    "            i = 0\n",
    "            while i < len(line):\n",
    "                if line[i].encode().isalpha() and line[i+2] == ')':\n",
    "                    embed.append(count)\n",
    "                    count = 0\n",
    "                    i += 2\n",
    "                elif line[i] == '(':\n",
    "                    count += 1\n",
    "                i += 1\n",
    "    return np.array(embed).reshape(len(embed), 1)\n",
    "\n",
    "parsed_root = 'data/parsed_text/'\n",
    "node_count_root = 'data/word_feature/'\n",
    "files = ['story_1_zh.txt', 'story_1_en.txt']\n",
    "for f in files:\n",
    "    nc_bu = node_count_bu(parsed_root+f)\n",
    "    scio.savemat(node_count_root+f[:-4]+'_bu.mat', {'data':nc_bu})\n",
    "    nc_td = node_count_td(parsed_root+f)\n",
    "    scio.savemat(node_count_root+f[:-4]+'_td.mat', {'data':nc_td})\n",
    "    \n",
    "print('computing node count... Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0945612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1],\n",
       "        [ 1],\n",
       "        [ 1],\n",
       "        ...,\n",
       "        [14],\n",
       "        [ 1],\n",
       "        [ 5]]),\n",
       " (1086, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc_bu, nc_bu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bf652a",
   "metadata": {},
   "source": [
    "### To align with fMRI data, the generated node count features should be convolved with HRF and downsampled to the same rate as fMRI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340962fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.8/site-packages/nilearn/glm/__init__.py:55: FutureWarning: The nilearn.glm module is experimental. It may change in any future release of Nilearn.\n",
      "  warn('The nilearn.glm module is experimental. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convolving node count... Done\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as scio\n",
    "import hdf5storage as hdf5\n",
    "import numpy as np\n",
    "from nilearn import glm\n",
    "import h5py\n",
    "\n",
    "zs = lambda v: (v-v.mean())/v.std()\n",
    "\n",
    "def convolve_feature(nc_file, time_file, out_file, ref_length, hrf):\n",
    "    data = scio.loadmat(nc_file)\n",
    "    data = data['data']\n",
    "    word_time = scio.loadmat(time_file)\n",
    "    start_time = word_time['start']\n",
    "    end_time = word_time['end']\n",
    "    length = int(end_time[0][-1]*100)\n",
    "    time_series = np.zeros([length])\n",
    "    t = 0\n",
    "    for j in range(length):\n",
    "        if t == data.shape[0]:\n",
    "            break\n",
    "        if j == int(start_time[0][t]*100):\n",
    "            for k in range(j, int(end_time[0][t]*100)):\n",
    "                time_series[k] += data[t]\n",
    "            while(j == int(start_time[0][t]*100)):\n",
    "                t += 1\n",
    "                if t == data.shape[0]:\n",
    "                    break\n",
    "    conv_series = np.convolve(hrf, time_series)\n",
    "    conv_series = conv_series[:length]\n",
    "    conv_series_ds = [conv_series[j] for j in range(0, length, 71)]\n",
    "    conv_series_ds = np.array(conv_series_ds)\n",
    "    # dump first 19 TRs because they are blank\n",
    "    word_feature = zs(conv_series_ds[19:ref_length+19].reshape(ref_length,1))\n",
    "    tmp = {'word_feature':word_feature.astype('float32')}\n",
    "    hdf5.writes(tmp, out_file, matlab_compatible=True)\n",
    "    \n",
    "node_count_root = 'data/word_feature/'\n",
    "time_root = 'data/time_feature/'\n",
    "convolved_root = 'data/convolved_feature/'\n",
    "fmri_root = 'data/fmri/'\n",
    "files = ['story_1_zh', 'story_1_en']\n",
    "hrf = glm.first_level.spm_hrf(0.71, 71)\n",
    "\n",
    "for f in files:\n",
    "    ref_length = h5py.File(fmri_root+f+'.mat')['fmri_response'].shape[0]\n",
    "    convolve_feature(node_count_root+f+'_bu.mat', time_root+f+'.mat', convolved_root+f+'_bu.mat', ref_length, hrf)\n",
    "    convolve_feature(node_count_root+f+'_td.mat', time_root+f+'.mat', convolved_root+f+'_td.mat', ref_length, hrf)\n",
    "    \n",
    "print('convolving node count... Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148654d3",
   "metadata": {},
   "source": [
    "### fMRI voxel-wise encoding experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c65ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "import itertools as itools\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "\n",
    "zs = lambda v: (v-v.mean(0))/v.std(0)\n",
    "\n",
    "class glm:\n",
    "    \"\"\" perform voxel-wise encoding \"\"\"\n",
    "    def __init__(self, result_dir, block_shuffle=False):\n",
    "        self.result_dir = result_dir\n",
    "        self.block_shuffle = block_shuffle\n",
    "\n",
    "    def run_glm(self, fmri, feature):\n",
    "        nTR, n_vox = fmri.shape\n",
    "        _, n_feature = feature.shape\n",
    "        if self.block_shuffle:\n",
    "            print('shuffling data with blocks...')\n",
    "            inds = self.block_shuffle_inds(nTR)\n",
    "            feature = feature[inds]\n",
    "            fmri = fmri[inds]\n",
    "        \n",
    "        W = np.zeros([n_feature, n_vox])\n",
    "\n",
    "        for i in tqdm(range(n_vox)):\n",
    "            model = sm.OLS(fmri[:, i], feature)\n",
    "            results = model.fit()\n",
    "            W[:,i] = results.params\n",
    "\n",
    "        savefile = self.result_dir+'glm_weight.mat'\n",
    "        scio.savemat(savefile, {'weight':W})\n",
    "\n",
    "    def block_shuffle_inds(self, nTR):\n",
    "        allinds = range(nTR)\n",
    "        blocklen = 100\n",
    "        indblocks = list(zip(*[iter(allinds)]*blocklen))\n",
    "        if nTR%blocklen != 0:\n",
    "            indblocks.append(range(len(indblocks)*blocklen, nTR))\n",
    "        random.shuffle(indblocks)\n",
    "        return list(itools.chain(*indblocks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18dbce0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(445, 59412) (445, 1)\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# load fmri\n",
    "fmri = h5py.File('data/fmri/story_1_en.mat', 'r')['fmri_response'][()]\n",
    "\n",
    "# load feature\n",
    "feature = h5py.File('data/convolved_feature/story_1_en_bu.mat', 'r')['word_feature'][()].T\n",
    "\n",
    "print(fmri.shape, feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "280f99fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 59412/59412 [00:17<00:00, 3337.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLM model analysis... Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. fit GLM model\n",
    "result_dir = 'results/'\n",
    "glm_model = glm(result_dir)\n",
    "glm_model.run_glm(fmri, feature)\n",
    "\n",
    "print('GLM model analysis... Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ad8af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
